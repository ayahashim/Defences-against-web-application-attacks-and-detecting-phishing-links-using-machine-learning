# -*- coding: utf-8 -*-
"""phishing-lstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J5COqoa1qpFsx8riHFU90MxUTHpCRezw
"""

# as you can see current ram is 12GB only, to increase it run below code

import csv
import random
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.layers.embeddings import Embedding
from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer
from sklearn.metrics import confusion_matrix

X=[]
Y=[]
with open("/content/all-data.csv" ,'r') as f:
  data = csv.reader(f)
  next(data)
  for row in data:
    url= row[1]
    if row[-1] == '0':
      label= 0
    elif row[-1] == '1' :
      label=1

    X.append(url)
    Y.append(label)

Y = np.array(Y)
Y = Y.reshape(Y.shape[0],1)
print('all data:', len(X) , X[0])
print(Y.shape , Y[0])

x_train = X[1458:]
Y_train = Y[1458:]
tokenizer = Tokenizer(filters='\t\n', char_level=True)
tokenizer.fit_on_texts(x_train)

word_index = tokenizer.word_index
print(word_index)

num_words = len(word_index) + 1
X_train = tokenizer.texts_to_sequences(x_train)
print ('example:',X[0])
max_length = 2083
X_train = pad_sequences(X_train, maxlen=max_length , padding = 'pre')

X_train = X_train.astype(float)
Y_train= Y_train.astype(float)

model = Sequential()
model.add(Embedding(num_words, 64, input_length=max_length))
model.add(LSTM(32, return_sequences=True))
model.add(LSTM(64,return_sequences=True))
model.add(LSTM(128 ))
model.add(Dense(20, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

model.fit(X_train, Y_train, epochs=20 , batch_size= 128)

x_train = X[0:729] 
Y_train = Y[0:729], Y[1458:]
for i in range (1458, len(X)):
  x_train.append(X[i])
Y_train = np.array(Y_train).reshape(1458,1)
Y_train.shape

tokenizer = Tokenizer(filters='\t\n', char_level=True)
tokenizer.fit_on_texts(x_train)

word_index = tokenizer.word_index
print(word_index)

num_words = len(word_index) + 1
X_train = tokenizer.texts_to_sequences(x_train)
print ('example:',x_train[0])
max_length = 2083
X_train = pad_sequences(X_train, maxlen=max_length , padding = 'pre')

X_train = X_train.astype(float)
Y_train= Y_train.astype(float)
print(X_train.shape, Y_train.shape)

model.fit(X_train, Y_train, epochs=20 , batch_size= 128)

tokenizer = Tokenizer(filters='\t\n', char_level=True)
tokenizer.fit_on_texts(X)

word_index = tokenizer.word_index
print(word_index)

num_words = len(word_index) + 1
X_train = tokenizer.texts_to_sequences(X)
print ('example:',X[0])
max_length = 2083
X_train = pad_sequences(X_train, maxlen=max_length , padding = 'pre')

X_train = X_train.astype(float)
Y_train= Y.astype(float)
print(X_train.shape, Y_train.shape)

model.fit(X_train, Y_train, epochs=20 , batch_size= 128)